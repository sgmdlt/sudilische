import asyncio
import csv
import json
import logging
import os
import random
import re
from datetime import datetime
from pathlib import Path

from bs4 import BeautifulSoup

from courts.captcha.handler import CaptchaHandler
from courts.downloader.async_downloader import Downloader
from courts.url import ParamsDTO, get_url

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

# –ó–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
HEADERS = {
    "User-Agent":
    "Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) " +
    "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Mobile Safari/537.36"
}
setattr(asyncio.sslproto._SSLProtocolTransport, "_start_tls_compatible", True)

with open('proxy.txt', 'r') as f:
    PROXIES_LIST = [line.strip() for line in f if line.strip()]

# captcha mock
class CaptchaSolver:
    async def solve_captcha(self, court_link, downloader):
        url = court_link + "name=sud_delo&name_op=sf&delo_id=1540005"
        soup = BeautifulSoup(await downloader.send_request(url), "html.parser")
        captcha_result = "12345"
        return {
            "captcha": captcha_result,
            "captchaid": soup.select_one("input[name=captchaid]")["value"]
        }

# –û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å—Ç—Ä–æ–∫ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –ø–æ–∏—Å–∫–∞
def parse_row(row, court):
    parsed_row = {}

    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –Ω—É–∂–Ω—ã–µ –ø–æ–ª—è
    parsed_row["region"] = court["region"]
    parsed_row["court_name"] = court["name"]
    parsed_row["case_number"] = row.select("td")[0].get_text(" ", strip=True)
    parsed_row["date_reg"] = row.select("td")[1].get_text(" ", strip=True)

    # –í—ã–¥–µ–ª—è–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –∫–∞—Ä—Ç–æ—á–∫—É –∏–∑ —Å—Ç—Ä–æ–∫–∏
    parsed_row["link"] = court["link"][:court["link"].find(
        "/modules")] + row.select("td")[0].select_one("a")["href"]

    # –°–æ–∑–¥–∞–µ–º –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä
    id_date = "NONE"
    if parsed_row["date_reg"]:
        id_date = datetime.strptime(parsed_row["date_reg"],
                                    "%d.%m.%Y").strftime("%Y-%m-%d")
    case_id = court["id"] + "_" + id_date + "_"
    try:
        if all(char not in parsed_row["case_number"]
               for char in ["(", "~", "‚àº"]):
            case_id += parsed_row["case_number"][:parsed_row["case_number"].
                                                 rfind("/")].replace("/", "-")
        elif id_date != "NONE" and parsed_row["date_reg"][-4:] in parsed_row[
                "case_number"]:
            case_id += re.search(r"([^ ~‚àº\(]+)/" + parsed_row["date_reg"][-4:],
                                 parsed_row["case_number"]).group(1).replace(
                                     "/", "-")
        else:
            case_id += re.search(r"([^ ~‚àº\(]+)/20",
                                 parsed_row["case_number"]).group(1).replace(
                                     "/", "-")
    except Exception:
        case_id += "ID_NOT_PARSED"

    parsed_row["ID"] = case_id

    return parsed_row


# –ì–ª–∞–≤–Ω—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–ø–∏—Å–∫–∞ —Å—É–¥–æ–≤
async def get_cases(task_number, courts, query_list, output_filename):

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—ã–≤–æ–¥ –∏ –∫–∞–ø—á—É
    failed_courts = []
    captcha_pack = {"captcha": "", "captchaid": "", "region": "00"}

    # –°–ª—É—á–∞–π–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è —Ä–∞–∑–≤–æ–¥–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
    await asyncio.sleep(random.uniform(1, 10))

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É–¥—ã –ø–æ —Å–ø–∏—Å–∫—É
    total_count = 0
    for court in courts:

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Å—ã–ª–∫—É
        for query_set in query_list:

            # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–µ—Å—Å–∏—é –∑–∞–Ω–æ–≤–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
            async with Downloader(proxies_list=PROXIES_LIST, captcha_handler=CaptchaHandler(CaptchaSolver())) as downloader:

                params = {'court_url': court["link"], 'court_type': '1', **query_set}
                url = get_url(ParamsDTO(**params))
                #url = court["link"] + get_url(court, query_set)

                # –¶–∏–∫–ª –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
                next_page = True
                card_count = 0
                target_count = 0
                while next_page:
                    # –ï—Å–ª–∏ —Ä–µ–≥–∏–æ–Ω —Ç–æ—Ç –∂–µ –∏ –∫–∞–ø—á—É –µ—â—ë –Ω–µ –ø—Ä–æ–≤–µ—Ä—è–ª–∏, —Å—Ç–∞–≤–∏–º —Å—Ç–∞—Ä—É—é
                    if "captcha" not in url and court["id"].startswith(
                            captcha_pack["region"]):
                        url += "&captcha=" + captcha_pack[
                            "captcha"] + "&captchaid=" + captcha_pack[
                                "captchaid"]

                    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å
                    response = await downloader.send_request(url)

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ—Ç –ª–∏ –æ—à–∏–±–∫–∏ –∫–∞–ø—á–∏
                    while any(error in response for error in [
                            "–ù–µ–≤–µ—Ä–Ω–æ —É–∫–∞–∑–∞–Ω –ø—Ä–æ–≤–µ—Ä–æ—á–Ω—ã–π –∫–æ–¥ —Å –∫–∞—Ä—Ç–∏–Ω–∫–∏",
                            "–í—Ä–µ–º—è –∂–∏–∑–Ω–∏ —Å–µ—Å—Å–∏–∏ –∑–∞–∫–æ–Ω—á–∏–ª–æ—Å—å"
                    ]):

                        # –†–µ—à–∞–µ–º –∫–∞–ø—á—É
                        print(f"ü§ñ {task_number:>2d} | ‚è≥ —Ä–µ—à–∞—é –∫–∞–ø—á—É...")
                        prev_captcha = captcha_pack["captchaid"]
                        got_new_captcha = False
                        while not got_new_captcha:
                            captcha_pack = await CaptchaSolver().solve_captcha(
                                court["link"], downloader)

                            # –ï—Å–ª–∏ –∫–∞–ø—á–∞ —É–∂–µ –æ–¥–∏–Ω —Ä–∞–∑ –Ω–µ –ø–æ–¥–æ—à–ª–∞, –∂–¥—ë–º –Ω–æ–≤—É—é
                            if prev_captcha == captcha_pack["captchaid"]:
                                print(
                                    f"ü§ñ {task_number:>2d} |",
                                    "‚òπÔ∏è –∫–∞–ø—á–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–∞ –Ω–µ—É–¥–∞—á–Ω–æ, –∂–¥—ë–º –Ω–æ–≤—É—é..."
                                )
                                await asyncio.sleep(20)
                            else:
                                got_new_captcha = True

                        captcha_pack.update({"region": court["id"][:2]})

                        # –£–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ä—É—é –∫–∞–ø—á—É
                        url = re.sub("&captcha=[^&]+", "", url)
                        url = re.sub("&captchaid=[^&]+", "", url)

                        # –î–æ–±–∞–≤–ª—è–µ–º –∫–∞–ø—á—É –≤ –∞–¥—Ä–µ—Å
                        url += "&captcha=" + captcha_pack[
                            "captcha"] + "&captchaid=" + captcha_pack[
                                "captchaid"]

                        # –ü—Ä–æ–±—É–µ–º –∫–æ–¥
                        await asyncio.sleep(1)
                        response = await downloader.send_request(url)

                    soup = BeautifulSoup(response.replace("</br>", "<br>"),
                                         "html.parser")

                    # –û–¥–∏–Ω —Ä–∞–∑ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º, —Å–∫–æ–ª—å–∫–æ –Ω–∞–¥–æ —Å–∫–∞—á–∞—Ç—å
                    if target_count == 0:
                        if target_str := re.search(
                                r"–í—Å–µ–≥–æ –ø–æ –∑–∞–ø—Ä–æ—Å—É –Ω–∞–π–¥–µ–Ω–æ [-‚Äî] (\d+)\.",
                                soup.get_text()):
                            target_count = int(target_str.group(1))

                    # –ü–µ—Ä–µ–±–æ—Ä —Ä—è–¥–æ–≤
                    for row in soup.select(
                            "table[id=tablcont] tr:not(:has(th)), " +
                            "div[id=resultTable] > table > tbody tr, " +
                            "div.wrapper-search-tables > table > tbody tr"):

                        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ —Ñ–∞–π–ª –ø–æ –∫–æ–¥—É —Å—É–¥–∞
                        output_row = parse_row(row, court)
                        open(output_filename + ".jsonl", "a").write(
                            json.dumps(output_row, ensure_ascii=False).replace(
                                u"\u00A0", " ") + "\n")


                        card_count += 1
                        total_count += 1

                    # –ü–µ—Ä–µ–±–æ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü
                    next_page = False
                    for link in soup.select(
                            "a[title='–°–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞'], " +
                            "ul.pagination a:-soup-contains-own('¬ª')"):
                        url = court["link"][:court["link"].
                                            find("/modules")] + link["href"][
                                                link["href"].find("/modules"):]

                        # –ï—Å–ª–∏ —Å–ª–µ—Ç–µ–ª —Ñ–ª–∞–≥ NC, –¥–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ
                        if "nc=1" in court["link"] and "nc=1" not in url:
                            url += "&nc=1"

                        next_page = True
                        break

                    # –û—Ç—á–∏—Ç—ã–≤–∞–µ–º—Å—è —á–µ—Ä–µ–∑ –∫–∞–∂–¥—ã–µ 500 –∫–∞—Ä—Ç–æ—á–µ–∫
                    if card_count % 500 == 0 and target_count > 0:
                        print(
                            f"ü§ñ {task_number:>2d} | {court['name']} üìá {card_count / target_count:>4.0%} –∫–∞—Ä—Ç–æ—á–µ–∫"
                        )

                print(f"ü§ñ {task_number:>2d}",
                      f"{court['name']:<40s}",
                      query_set["type"],
                      query_set["instance"],
                      query_set["entry_date_from"],
                      query_set["entry_date_to"],
                      end=" | ",
                      sep=" | ")

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                if card_count > 0 and card_count >= target_count:
                    print(f"üèÅ {card_count:>4d}")

                # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏, —Å—Ç–∞–≤–∏–º –Ω–æ–ª—å
                elif soup.select(
                        "div[id=content]:-soup-contains('–î–∞–Ω–Ω—ã—Ö –ø–æ –∑–∞–ø—Ä–æ—Å—É –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ'), "
                        +
                        "div[id=search_results]:-soup-contains('–î–∞–Ω–Ω—ã—Ö –ø–æ –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω–æ'), "
                        +
                        "div.resultsearch_text:-soup-contains('–ü–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ')"
                ):
                    print("ü´ô")

                # –ï—Å–ª–∏ —Å—É–¥ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
                elif soup.select(
                        "div.box:-soup-contains('–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞'), "
                        +
                        "body:-soup-contains('–∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω –ø–æ —Å–æ–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏')"
                ):
                    print("‚õîÔ∏è")
                    if court not in failed_courts:
                        failed_courts.append(court)

                # –ï—Å–ª–∏ —á—Ç–æ-—Ç–æ –Ω–µ–∑–Ω–∞–∫–æ–º–æ–µ
                else:
                    print("üü•üü•üü•üü•")
                    print(url)
                    if court not in failed_courts:
                        failed_courts.append(court)

    print(
        f"ü§ñ {task_number:>2d} | {'üü¢ –ó–ê–ö–û–ù–ß–ò–õ':>30s}: {total_count:>4d} –∫–∞—Ä—Ç–æ—á–µ–∫"
    )
    return total_count, failed_courts

# –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞
def remove_duplicates(filename):

    print("\n–£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –≤:", filename)

    unique_ids = set()
    count = 0
    with open(filename + ".jsonl", 'w+') as input_file:
        for line in input_file:
            case = json.loads(line)

            if case["ID"] not in unique_ids:
                open(filename + ".jsonl" + "c", "a+").write(line)
                unique_ids.add(case["ID"])
                count += 1

        input_file.close()
    if not Path.exists(Path(f'./{filename}.jsonlc').absolute()):
        open(f'{filename}.jsonlc', 'w+')
    os.rename(filename + ".jsonl" + "c", filename + ".jsonl")

    print("–ë–µ–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∑–∞–ø–∏—Å–∞–Ω–æ:", count)


# –ó–∞–ø—É—Å–∫ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—É–¥–æ–≤
async def court_runner(output_filename, court_codes, query_set, tasks):

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å—É–¥–æ–≤
    courts_list = []
    if "Failed" in court_codes:
        courts_list.extend(list(csv.DictReader(open(court_codes + ".csv"))))
    else:
        for court in csv.DictReader(open("./court_list_all.csv")):
            if court["id"][2:4] in court_codes.split("+") and court[
                    "id"] != "77RS0000" and court["id"] != "77OS0000":
                courts_list.append(court)
    print("–í —Å–ø–∏—Å–∫–µ —Å—É–¥–æ–≤", court_codes, len(courts_list), "—Å—Ç—Ä–æ–∫")

    # –°–æ–∑–¥–∞–µ–º –ø–µ—Ä–µ—á–µ–Ω—å –ø—Ä–æ—Ü–µ—Å—Å–æ–≤
    coro_list = []
    batch_size = round(len(courts_list) / tasks)
    print("–°—É–¥–æ–≤ –Ω–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ü–µ—Å—Å:", batch_size, "\n\n")
    for i in range(tasks):
        if i < tasks - 1:
            courts_batch = courts_list[i * batch_size:i * batch_size +
                                       batch_size]
        else:
            courts_batch = courts_list[i * batch_size:]
        coro_list.append(
            get_cases(i + 1, courts_batch, query_set, output_filename))

    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ –ø—Ä–æ—Ü–µ—Å—Å—ã
    gather_start = datetime.now()
    gather_result = await asyncio.gather(*coro_list)

    # –ü–æ—Å–ª–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è —Å–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –æ–±—â–∏–π —Å–ø–∏—Å–æ–∫
    final_count = 0
    final_failed = []
    for total_count, failed_courts in gather_result:
        final_count += total_count
        final_failed.extend(failed_courts)

    print("\n\n–ó–∞–ø–∏—Å–∞–Ω–æ –≤ —Ñ–∞–π–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:", final_count)

    # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    #remove_duplicates(output_filename)

    # –ó–∞–ø–∏—Å—å —Å—É–¥–æ–≤ —Å –æ—à–∏–±–∫–∞–º–∏
    if final_failed:
        with open(output_filename + "-Failed.csv", 'w+') as failed_output:
            writer = csv.DictWriter(failed_output, courts_list[0].keys())
            writer.writeheader()
            writer.writerows(final_failed)
        print("\n–ó–∞–ø–∏—Å–∞–Ω–æ —Å—É–¥–æ–≤ —Å –æ—à–∏–±–∫–∞–º–∏:", len(final_failed))

    print("–ü—Ä–æ—à–ª–æ", int(
        (datetime.now() - gather_start).total_seconds() / 3600), "—á–∞—Å(–æ–≤)")


# –ì–õ–ê–í–ù–´–ô –ó–ê–ü–£–°–ö

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–∫—Å–∏
PROXIES_LIST = open("./proxy.txt").read().splitlines()[1:]

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
""" loaded_model = load_model("sud_rnn_0.6.1")
prediction_model = models.Model(
    loaded_model.get_layer(name="image").input,
    loaded_model.get_layer(name="dense2").output)
characters = "0123456789"
char_to_num = layers.StringLookup(vocabulary=list(characters), mask_token=None)
num_to_char = layers.StringLookup(vocabulary=char_to_num.get_vocabulary(),
                                  mask_token=None,
                                  invert=True) """

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
QUERY_SET = {
    "type": "",
    "instance": 1,
    "article": "",
    "participant": "",
    "entry_date_from": "",
    "entry_date_to": "",
    "mat_category": ["", "", ""],
    "download_mat_cat": False
}

article_collection = {
    1: ["–£–ö", "–ì–ö", "–ö–ê–°", "–ö–æ–ê–ü", "–ú–∞—Ç–µ—Ä–∏–∞–ª"],
    2: ["–£–ö", "–ì–ö", "–ö–ê–°", "–ö–æ–ê–ü"]
}

# –ù–∞–±–æ—Ä –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ —Å —Ä–∞–∑–±–∏–µ–Ω–∏–µ–º –ø–æ –º–µ—Å—è—Ü–∞
query_list = []
for number in [1, 2]:
    QUERY_SET["instance"] = number
    for art in article_collection[number]:
        QUERY_SET["type"] = str(art)

        QUERY_SET["entry_date_from"] = ""
        QUERY_SET["entry_date_to"] = "01.01.2010"
        query_list.append(QUERY_SET.copy())
        current_year = datetime.now().year
        current_month = datetime.now().month
        for year in range(2010, current_year):
            for month in range(1, 12):
                QUERY_SET["entry_date_from"] = f"01.{month:02d}.{year}"
                QUERY_SET["entry_date_to"] = f"01.{month + 1:02d}.{year}"
                query_list.append(QUERY_SET.copy())
            QUERY_SET["entry_date_from"] = f"01.12.{year}"
            QUERY_SET["entry_date_to"] = f"01.01.{year + 1}"
            query_list.append(QUERY_SET.copy())

        for month in range(1, current_month):
            QUERY_SET["entry_date_from"] = f"01.{month:02d}.{current_year}"
            QUERY_SET["entry_date_to"] = f"01.{month + 1:02d}.{current_year}"
            query_list.append(QUERY_SET.copy())
        QUERY_SET["entry_date_from"] = f"01.{current_month:02d}.{current_year}"
        QUERY_SET["entry_date_to"] = ""
        query_list.append(QUERY_SET.copy())

# –ó–∞–ø—É—Å–∫ —Å–±–æ—Ä–∞
# –ò–º—è —Ñ–∞–π–ª–∞ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞, –Ω–∞–±–æ—Ä —Å—É–¥–æ–≤, —Å–ø–∏—Å–æ–∫ (!) –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤
#asyncio.run(court_runner("GV-All-8", "RS", query_list, 4))
